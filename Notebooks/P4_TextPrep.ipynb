{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metis Data Science Bootcamp\n",
    "## San Francisco, Winter 2020\n",
    "### Project 4: Election Reporting Sentiment Analysis\n",
    "\n",
    "### Text Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os.path\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from os import path\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = MongoClient()\n",
    "db_news = db_client['news_search']\n",
    "db_news_col = db_news['search_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_news_content = db_news['news_content']\n",
    "cursor = db_news_content.find({}, {'_id':0, 'name': 1, 'text': 1, 'url' : 1, 'base_url' : 1, 'pub_date': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795\n"
     ]
    }
   ],
   "source": [
    "cursor = db_news_content.find({}, {'_id':0, 'name': 1, 'text': 1, 'url' : 1, 'base_url' : 1, 'pub_date': 1})\n",
    "print(len(list(cursor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation+'\\\\'), ' ', x.lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_news_content = db_news['news_content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entitity Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795 2844063\n"
     ]
    }
   ],
   "source": [
    "db_news_content = db_news['news_content']\n",
    "\n",
    "cursor = db_news_content.find({}, {'_id':0, 'name': 1, 'text': 1, 'url' : 1, 'base_url' : 1, 'pub_date': 1})\n",
    "article_count = 0\n",
    "total_words = 0\n",
    "\n",
    "for article in list(cursor):\n",
    "    word_list = []\n",
    "    article_count += 1\n",
    "    clean_text = ''\n",
    "    clean_text = punc_lower(alphanumeric(article['text']))\n",
    "    clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "    clean_text = article['text']\n",
    "    word_list = clean_text.split()\n",
    "    # print(article['name'], len(word_list))\n",
    "    total_words += len(word_list)\n",
    "    \n",
    "print(article_count, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's clean the text\n",
    "cursor = db_news_content.find({}, {'_id':1, 'name': 1, 'text': 1, 'url' : 1, 'base_url' : 1, 'pub_date': 1})\n",
    "db_news_sentences = db_news['sentences']\n",
    "for article in list(cursor) :\n",
    "    # make alphanumeric and lower case\n",
    "    clean_text = punc_lower(alphanumeric(article['text']))\n",
    "    \n",
    "    # remove numbers\n",
    "    clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', clean_text)\n",
    "    clean_text = re.sub('\\\\xa0', ' ', clean_text)\n",
    "    \n",
    "    # word_list = clean_text.split()\n",
    "    word_list = [ x for x in word_list if (len(x) >= 2) ]\n",
    "    db_news_content.update_one({ '_id': article['_id']}, { '$set' : { 'clean_text' : clean_text }})\n",
    "    db_news_content.update_one({ '_id': article['_id']}, { '$set' : { 'word_list' : word_list }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db_news_content.find({}, {'_id':1, 'text': 1, 'clean_text' : 1 })\n",
    "articles = list(cursor)\n",
    "\n",
    "'''\n",
    "ADJ: adjective, e.g. big, old, green, incomprehensible, first\n",
    "ADP: adposition, e.g. in, to, during\n",
    "ADV: adverb, e.g. very, tomorrow, down, where, there\n",
    "AUX: auxiliary, e.g. is, has (done), will (do), should (do)\n",
    "CONJ: conjunction, e.g. and, or, but\n",
    "CCONJ: coordinating conjunction, e.g. and, or, but\n",
    "DET: determiner, e.g. a, an, the\n",
    "INTJ: interjection, e.g. psst, ouch, bravo, hello\n",
    "NOUN: noun, e.g. girl, cat, tree, air, beauty\n",
    "NUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV\n",
    "PART: particle, e.g. â€™s, not,\n",
    "PRON: pronoun, e.g I, you, he, she, myself, themselves, somebody\n",
    "PROPN: proper noun, e.g. Mary, John, London, NATO, HBO\n",
    "PUNCT: punctuation, e.g. ., (, ), ?\n",
    "SCONJ: subordinating conjunction, e.g. if, while, that\n",
    "SYM: symbol, e.g. $, %, Â§, Â©, +, âˆ’, Ã—, Ã·, =, :), ðŸ˜\n",
    "VERB: verb, e.g. run, runs, running, eat, ate, eating\n",
    "X: other, e.g. sfpksdpsxmsa\n",
    "SPACE: space, e.g.\n",
    "'''\n",
    "\n",
    "'''\n",
    "PERSON\tPeople, including fictional.\n",
    "NORP\tNationalities or religious or political groups.\n",
    "FAC\tBuildings, airports, highways, bridges, etc.\n",
    "ORG\tCompanies, agencies, institutions, etc.\n",
    "GPE\tCountries, cities, states.\n",
    "LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "WORK_OF_ART\tTitles of books, songs, etc.\n",
    "LAW\tNamed documents made into laws.\n",
    "LANGUAGE\tAny named language.\n",
    "DATE\tAbsolute or relative dates or periods.\n",
    "TIME\tTimes smaller than a day.\n",
    "PERCENT\tPercentage, including â€%â€œ.\n",
    "MONEY\tMonetary values, including unit.\n",
    "QUANTITY\tMeasurements, as of weight or distance.\n",
    "ORDINAL\tâ€œfirstâ€, â€œsecondâ€, etc.\n",
    "CARDINAL\tNumerals that do not fall under another type.\n",
    "'''\n",
    "\n",
    "#pos_keep_list = ['ADJ', 'ADV', 'NOUN', 'PROPN', 'VERB']\n",
    "pos_keep_list = ['ADJ','ADV', 'NOUN', 'VERB']\n",
    "# ent_keep_list = ['PERSON', 'ORG', 'NORP', 'GPE', '']\n",
    "ent_keep_list = ['PERSON', 'NORP', 'EVENT', 'PRODUCT', 'GPE', 'MONEY', 'QUANTITY', '']\n",
    "'''\n",
    "people_to_drop = ['tom', 'steyer', 'bernie','sanders', 'sander', 'donald', 'trump', 'elizabeth', 'warren', \\\n",
    "                  'joe','biden', 'mike', 'bloomberg',  \\\n",
    "                   'pete','buttigieg', 'amy',  'klobuchar']\n",
    "                   \n",
    "                   \n",
    "'''\n",
    "\n",
    "people_to_drop = ['tom', 'bernie', 'donald',  'elizabeth', 'joe',  'michael', 'pete', 'amy']\n",
    "\n",
    "words_to_drop = ['president', 'hall', 'senator', 'vice', 'mayor', 'moines', 'democrat', 'democratic', \\\n",
    "                 'president', 'presidential', 'south', 'bend', 'candidate', 'state', 'this', 'look', 'make',\\\n",
    "                 'think', 'that', 'what', 'like', 'campaign', 'know' ]\n",
    "\n",
    "for article in articles :\n",
    "    word_list = []\n",
    "    # Tag the parts of speech\n",
    "    doc = nlp(article['clean_text'])\n",
    "\n",
    "    # tokenize the text\n",
    "    '''\n",
    "    word_list = [token.lemma_  \\\n",
    "                 if ((token.pos_ in pos_keep_list) and \\\n",
    "                    (token.ent_type_ in ent_keep_list) and (token.text in nlp.vocab) \\\n",
    "                     and not (token.text in people_to_drop) and not (token.text in words_to_drop)) \\\n",
    "                 else 'location' if (token.ent_type_ == 'GPE') else '' for token in doc ]\n",
    "    '''\n",
    "    prop_noun_list = [token.text for token in doc if (token.pos_ == 'PROPN') ]\n",
    "    word_list = [token.lemma_  \\\n",
    "                 if ((token.pos_ in pos_keep_list) or \\\n",
    "                    (token.ent_type_ in ent_keep_list)) and (token.text in nlp.vocab) else ' ' for token in doc ]\n",
    "    \n",
    "    word_list = [word for word in word_list if word.isalpha() and (len(word) > 3)]\n",
    "    word_list = [word for word in word_list if not word in people_to_drop]\n",
    "    word_list = [word for word in word_list if not word in words_to_drop]\n",
    "    \n",
    "    # make word substitutions for names\n",
    "    '''\n",
    "    for i, word in enumerate(word_list) :\n",
    "        new_word = word\n",
    "        if word == 'donald' :\n",
    "            new_word = 'trump'\n",
    "        elif word == 'michael' :\n",
    "            new_word = 'bloomberg'\n",
    "        elif word == 'elizabeth' :\n",
    "            new_word = 'warren'\n",
    "        elif word == 'pete' :\n",
    "            new_word = 'buttigieg'\n",
    "        elif word == 'joe':\n",
    "            new_word = 'biden'\n",
    "        elif word == 'bernie' :\n",
    "            new_word = 'sanders'\n",
    "        elif word == 'amy' :\n",
    "            new_word = 'klobuchar'\n",
    "        elif word == 'tom' :\n",
    "            new_word = 'steyer'\n",
    "        word_list[i] = new_word\n",
    "    '''\n",
    "        \n",
    "    short_text = ' '.join(word_list)\n",
    "    prop_noun_text = ' '.join(prop_noun_list)\n",
    "    \n",
    "    db_news_content.update_one({'_id': article['_id']}, { '$set' : {'short_text' : short_text}})\n",
    "    db_news_content.update_one({'_id': article['_id']}, { '$set' : {'prop_nouns' : prop_noun_text}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix datetime\n",
    "import datetime as dt\n",
    "cursor = db_news_content.find({}, {'_id':1, 'text': 1, 'pub_date' : 1 })\n",
    "articles = list(cursor)\n",
    "\n",
    "for article in articles :\n",
    "    dt_pub_date = dt.datetime.strptime(article['pub_date'].split('T')[0], '%Y-%m-%d')\n",
    "    db_news_content.update_one({'_id': article['_id']}, { '$set' : {'dt_pub_date' : dt_pub_date}})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 2, 19, 0, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = db_news_content.find({}, {'_id':1, 'text': 1, 'pub_date' : 1, 'dt_pub_date' : 1 })\n",
    "articles = list(cursor)\n",
    "articles[2]['dt_pub_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's clean the text\n",
    "cursor = db_news_content.find({}, {'_id':1, 'name': 1, 'text': 1, 'clean_text' : 1})\n",
    "for article in list(cursor) :\n",
    "    for sent in doc.sents:\n",
    "        blob1 = TextBlob(article['text'])\n",
    "        db_news_content.update_one({'_id': article['_id']},{ '$set' : {'sentiment': blob1.sentiment}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nIndexesWas': 1, 'ns': 'news_search.sentences', 'ok': 1.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_news.drop_collection('sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
