{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Media Topic Tracking</h1>\n",
    "<h2 align=center>Using Natural Language Processing</h2>\n",
    "<h2 align=center>and</h2>\n",
    "<h2 align=center>Machine Learning</h2>\n",
    "\n",
    "### Text Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant news articles are identified and scraped using the script news_scrape.py script.  Once they are scraped they are stored in their raw from in a MongoDB collection.  The contents of this notebook does the initial cleaning of the data and stores the cleaned text back in the same MongoDB collection along with the other document identifiers and raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules we will need\n",
    "import sys\n",
    "import re\n",
    "import os.path\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from os import path\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Generate some quick summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open MongoDB\n",
    "db_client = MongoClient()\n",
    "\n",
    "# Get set up access to the collections we will need here\n",
    "db_news = db_client['news_search']\n",
    "db_news_content = db_news['news_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all the documents and print the count\n",
    "doc_count = db_news_content.count_documents({})\n",
    "print(doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define a couple of useful functions for cleaning text\n",
    "#\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation+'\\\\'), ' ', x.lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795 2844063\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of words in the clean corpus.\n",
    "\n",
    "cursor = db_news_content.find({}, {'_id':0, 'name': 1, 'text': 1, 'url' : 1, 'base_url' : 1, 'pub_date': 1})\n",
    "article_count = 0\n",
    "total_words = 0\n",
    "\n",
    "for article in list(cursor):\n",
    "    word_list = []\n",
    "    article_count += 1\n",
    "    clean_text = ''\n",
    "    clean_text = punc_lower(alphanumeric(article['text']))\n",
    "    clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "    clean_text = article['text']\n",
    "    word_list = clean_text.split()\n",
    "    # print(article['name'], len(word_list))\n",
    "    total_words += len(word_list)\n",
    "    \n",
    "print(article_count, total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the text\n",
    "\n",
    "The first steps in cleaning are removing punctuation, stripping number and making everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's clean the text\n",
    "cursor = db_news_content.find({}, {'_id':1, 'name': 1, 'text': 1, 'url' : 1, 'base_url' : 1, 'pub_date': 1})\n",
    "db_news_sentences = db_news['sentences']\n",
    "for article in list(cursor) :\n",
    "    # make alphanumeric and lower case\n",
    "    clean_text = punc_lower(alphanumeric(article['text']))\n",
    "    \n",
    "    # remove numbers\n",
    "    clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', clean_text)\n",
    "    clean_text = re.sub('\\\\xa0', ' ', clean_text)\n",
    "    \n",
    "    # word_list = clean_text.split()\n",
    "    word_list = [ x for x in word_list if (len(x) >= 2) ]\n",
    "    db_news_content.update_one({ '_id': article['_id']}, { '$set' : { 'clean_text' : clean_text }})\n",
    "    db_news_content.update_one({ '_id': article['_id']}, { '$set' : { 'word_list' : word_list }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Of Speech Filtering\n",
    "Once the basic cleaning is done we do some more advanced filtering based on parts of speech and Named Entity Recognition.\n",
    "Basic stop words are removed and words are converted to their lemma form.  After that we apply some simple word filters to remove\n",
    "words that are effectively stop words for the domain in which we are working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db_news_content.find({}, {'_id':1, 'text': 1, 'clean_text' : 1 })\n",
    "articles = list(cursor)\n",
    "\n",
    "'''\n",
    "ADJ: adjective, e.g. big, old, green, incomprehensible, first\n",
    "ADP: adposition, e.g. in, to, during\n",
    "ADV: adverb, e.g. very, tomorrow, down, where, there\n",
    "AUX: auxiliary, e.g. is, has (done), will (do), should (do)\n",
    "CONJ: conjunction, e.g. and, or, but\n",
    "CCONJ: coordinating conjunction, e.g. and, or, but\n",
    "DET: determiner, e.g. a, an, the\n",
    "INTJ: interjection, e.g. psst, ouch, bravo, hello\n",
    "NOUN: noun, e.g. girl, cat, tree, air, beauty\n",
    "NUM: numeral, e.g. 1, 2017, one, seventy-seven, IV, MMXIV\n",
    "PART: particle, e.g. ‚Äôs, not,\n",
    "PRON: pronoun, e.g I, you, he, she, myself, themselves, somebody\n",
    "PROPN: proper noun, e.g. Mary, John, London, NATO, HBO\n",
    "PUNCT: punctuation, e.g. ., (, ), ?\n",
    "SCONJ: subordinating conjunction, e.g. if, while, that\n",
    "SYM: symbol, e.g. $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù\n",
    "VERB: verb, e.g. run, runs, running, eat, ate, eating\n",
    "X: other, e.g. sfpksdpsxmsa\n",
    "SPACE: space, e.g.\n",
    "'''\n",
    "\n",
    "'''\n",
    "PERSON\tPeople, including fictional.\n",
    "NORP\tNationalities or religious or political groups.\n",
    "FAC\tBuildings, airports, highways, bridges, etc.\n",
    "ORG\tCompanies, agencies, institutions, etc.\n",
    "GPE\tCountries, cities, states.\n",
    "LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "WORK_OF_ART\tTitles of books, songs, etc.\n",
    "LAW\tNamed documents made into laws.\n",
    "LANGUAGE\tAny named language.\n",
    "DATE\tAbsolute or relative dates or periods.\n",
    "TIME\tTimes smaller than a day.\n",
    "PERCENT\tPercentage, including ‚Äù%‚Äú.\n",
    "MONEY\tMonetary values, including unit.\n",
    "QUANTITY\tMeasurements, as of weight or distance.\n",
    "ORDINAL\t‚Äúfirst‚Äù, ‚Äúsecond‚Äù, etc.\n",
    "CARDINAL\tNumerals that do not fall under another type.\n",
    "'''\n",
    "\n",
    "#pos_keep_list = ['ADJ', 'ADV', 'NOUN', 'PROPN', 'VERB']\n",
    "pos_keep_list = ['ADJ','ADV', 'NOUN', 'VERB']\n",
    "# ent_keep_list = ['PERSON', 'ORG', 'NORP', 'GPE', '']\n",
    "ent_keep_list = ['PERSON', 'NORP', 'EVENT', 'PRODUCT', 'GPE', 'MONEY', 'QUANTITY', '']\n",
    "'''\n",
    "people_to_drop = ['tom', 'steyer', 'bernie','sanders', 'sander', 'donald', 'trump', 'elizabeth', 'warren', \\\n",
    "                  'joe','biden', 'mike', 'bloomberg',  \\\n",
    "                   'pete','buttigieg', 'amy',  'klobuchar']\n",
    "                   \n",
    "                   \n",
    "'''\n",
    "\n",
    "people_to_drop = ['tom', 'bernie', 'donald',  'elizabeth', 'joe',  'michael', 'pete', 'amy']\n",
    "\n",
    "words_to_drop = ['president', 'hall', 'senator', 'vice', 'mayor', 'moines', 'democrat', 'democratic', \\\n",
    "                 'president', 'presidential', 'south', 'bend', 'candidate', 'state', 'this', 'look', 'make',\\\n",
    "                 'think', 'that', 'what', 'like', 'campaign', 'know' ]\n",
    "\n",
    "for article in articles :\n",
    "    word_list = []\n",
    "    # Tag the parts of speech\n",
    "    doc = nlp(article['clean_text'])\n",
    "\n",
    "    # tokenize the text\n",
    "    '''\n",
    "    word_list = [token.lemma_  \\\n",
    "                 if ((token.pos_ in pos_keep_list) and \\\n",
    "                    (token.ent_type_ in ent_keep_list) and (token.text in nlp.vocab) \\\n",
    "                     and not (token.text in people_to_drop) and not (token.text in words_to_drop)) \\\n",
    "                 else 'location' if (token.ent_type_ == 'GPE') else '' for token in doc ]\n",
    "    '''\n",
    "    prop_noun_list = [token.text for token in doc if (token.pos_ == 'PROPN') ]\n",
    "    word_list = [token.lemma_  \\\n",
    "                 if ((token.pos_ in pos_keep_list) or \\\n",
    "                    (token.ent_type_ in ent_keep_list)) and (token.text in nlp.vocab) else ' ' for token in doc ]\n",
    "    \n",
    "    word_list = [word for word in word_list if word.isalpha() and (len(word) > 3)]\n",
    "    word_list = [word for word in word_list if not word in people_to_drop]\n",
    "    word_list = [word for word in word_list if not word in words_to_drop]\n",
    "    \n",
    "    # make word substitutions for names\n",
    "    '''\n",
    "    for i, word in enumerate(word_list) :\n",
    "        new_word = word\n",
    "        if word == 'donald' :\n",
    "            new_word = 'trump'\n",
    "        elif word == 'michael' :\n",
    "            new_word = 'bloomberg'\n",
    "        elif word == 'elizabeth' :\n",
    "            new_word = 'warren'\n",
    "        elif word == 'pete' :\n",
    "            new_word = 'buttigieg'\n",
    "        elif word == 'joe':\n",
    "            new_word = 'biden'\n",
    "        elif word == 'bernie' :\n",
    "            new_word = 'sanders'\n",
    "        elif word == 'amy' :\n",
    "            new_word = 'klobuchar'\n",
    "        elif word == 'tom' :\n",
    "            new_word = 'steyer'\n",
    "        word_list[i] = new_word\n",
    "    '''\n",
    "        \n",
    "    short_text = ' '.join(word_list)\n",
    "    prop_noun_text = ' '.join(prop_noun_list)\n",
    "    \n",
    "    db_news_content.update_one({'_id': article['_id']}, { '$set' : {'short_text' : short_text}})\n",
    "    db_news_content.update_one({'_id': article['_id']}, { '$set' : {'prop_nouns' : prop_noun_text}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Date Conversion\n",
    "Finally we're going to convert the publication date to a datetime object for later filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix datetime\n",
    "import datetime as dt\n",
    "cursor = db_news_content.find({}, {'_id':1, 'text': 1, 'pub_date' : 1 })\n",
    "articles = list(cursor)\n",
    "\n",
    "for article in articles :\n",
    "    dt_pub_date = dt.datetime.strptime(article['pub_date'].split('T')[0], '%Y-%m-%d')\n",
    "    db_news_content.update_one({'_id': article['_id']}, { '$set' : {'dt_pub_date' : dt_pub_date}})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
