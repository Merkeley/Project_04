{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Media Topic Tracking</h1>\n",
    "<h2 align=center>Using Natural Language Processing</h2>\n",
    "<h2 align=center>and</h2>\n",
    "<h2 align=center>Machine Learning</h2>\n",
    "\n",
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the text is cleaned we can start the process of topic modeling.  Topic modeling can be done in several different ways.  The first step in any approach is vectorizing the text using either an Count Vectorizer or Term Frequency - Inverse Document Frequency (TF-IDF) Vectorizer.  We then use one of several different approaches to topic identification.  The simplest is using Singular Value Decomposition on the term matrix (vectorizer output).  A more complicated approach is using Latent Dirichlet Allocation (LDA) which is a probablistic method for finding primary components in the term matrix.  Finally, we can also apply Primary Component Analysis to the term matrix and then apply a clustering algorithm to the output of PCA.<br>\n",
    "<br>\n",
    "In this notebook I examine SVD and LDA.  PCA and KMeans are evaluated in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os.path\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from os import path\n",
    "\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the document database\n",
    "db_client = MongoClient()\n",
    "db_news = db_client['news_search']\n",
    "db_news_col = db_news['search_result']\n",
    "db_news_content = db_news['news_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some utility functions\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=center>Vectorizing</h2>\n",
    "\n",
    "### Count Vectorizer\n",
    "As mentioned above, there are several vectorizing techniques.  This is the count\n",
    "vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build our corpus\n",
    "corpus = []\n",
    "cursor = db_news_content.find({}, {'_id':1, 'text': 1, 'short_text' : 1, 'prop_nouns' : 1})\n",
    "for article in list(cursor) :\n",
    "    corpus.append(article['short_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of articles for transformation and a list of associated article id's\n",
    "cursor = db_news_content.find({}, {'_id':1, 'text': 1, 'short_text' : 1, 'prop_nouns' : 1})\n",
    "X_what = []\n",
    "y = []\n",
    "articles = list(cursor)\n",
    "for article in articles :\n",
    "    X_what.append(article['short_text'])\n",
    "    y.append(article['_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=1.5, max_features=80, ngram_range=(1,2))\n",
    "cv = vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the article list and convert it to a data frame\n",
    "X_cv_what = cv.transform(X_what)\n",
    "X_cv_what_df = pd.DataFrame(X_cv_what.toarray(), columns=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "The second vectorizing technique is TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the vectorizer\n",
    "tf_vect = TfidfVectorizer(stop_words='english', max_df=1.5, max_features=80, ngram_range=(1,2))\n",
    "tfidf = tf_vect.fit(corpus)\n",
    "\n",
    "# Transform the document list and create a dataframe\n",
    "X_tf_what = tfidf.transform(X_what)\n",
    "X_tf_what_df = pd.DataFrame(X_cv_what.toarray(), columns=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=center>Topic Modeling</h2>\n",
    "\n",
    "Now that the corpus has been vectorized we'll look at the actual topic modeling techniques. \n",
    "\n",
    "<h3>LDA</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep some values for later visualization\n",
    "sum_words = X_cv_what.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq[:300], key = lambda x: x[1], reverse=True)\n",
    "top_words = [word[0].upper() for word in words_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "lda_model = LatentDirichletAllocation(n_components=8,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "\n",
    "# Apply the model to the count vectorized document df\n",
    "lda_output = lda_model.fit_transform(X_cv_what)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: debate steyer bloomberg billionaire million\n",
      "Topic #1: biden voter poll sander hampshire\n",
      "Topic #2: caucus iowa result delegate sander\n",
      "Topic #3: iowa buttigieg people event year\n",
      "Topic #4: buttigieg klobuchar black debate hampshire\n",
      "Topic #5: sander biden trump democrats year\n",
      "Topic #6: bloomberg trump york people city\n",
      "Topic #7: warren woman sander plan debate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda_model, vectorizer.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis is a useful visualization tool for LDA output.  The package applies \n",
    "PCA to the output of the model to make some useful visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maboals/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sklearn\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "ldavizdata = pyLDAvis.sklearn.prepare(lda_model, X_cv_what, cv, mds='tsne')\n",
    "pyLDAvis.save_html(ldavizdata, 'pyLDA_Full_Corpus.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the output above we can name the topics\n",
    "topic_what_labels = ['Sanders Trump', 'People', 'Biden', 'Bloomberg',\\\n",
    "                     'Iowa Caucus Result', 'Buttigieg', 'Voter Steyer', 'Warren']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Truncated Singular Value Decomposition</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.762987352441249"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_cv = TruncatedSVD(15)\n",
    "X_cv_topic_what = lsa_cv.fit_transform(X_cv_what)\n",
    "\n",
    "sum(lsa_cv.explained_variance_ratio_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
